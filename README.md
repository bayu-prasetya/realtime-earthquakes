
# ğŸŒ Earthquake ETL Pipeline (USGS API â†’ BigQuery â†’ Dashboard)

This project is an end-to-end ETL (Extract-Transform-Load) pipeline that retrieves real-time and historical global earthquake data from the **USGS Earthquake API**, processes it using Python, and uploads it into **Google BigQuery** for analysis and visualization (e.g., via **Looker Studio**).

---

## ğŸ¯ Project Objectives

- Build a robust data pipeline using Python
- Work with real-world data from public APIs
- Store data in a cloud warehouse (Google BigQuery)
- Visualize earthquake trends on dashboards
- Demonstrate both **real-time** and **historical** data handling

---

## ğŸ“¦ Features

- âœ… Extract earthquake data from USGS (24h, 1h, or by date)
- âœ… Transform and clean data using pandas
- âœ… Enrich with derived features (e.g., magnitude category)
- âœ… Upload to Google BigQuery with flexible upload mode
- âœ… Logging to file and console for monitoring
- âœ… Ready for dashboard integration (Looker Studio or Data Studio)

---

## ğŸ—‚ï¸ Project Structure

```
etl_project/
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ extract.py               # Fetch data from API
â”‚   â”œâ”€â”€ transform.py             # Clean & enrich data
â”‚   â””â”€â”€ load.py                  # Upload to BigQuery
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ etl_log_*.log            # Log files
â”œâ”€â”€ main.py                      # Orchestration script
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md                    # Project documentation
```

---

## ğŸ§ª Installation

1. **Clone repository**
   ```bash
   git clone https://github.com/your-username/earthquake-etl.git
   cd earthquake-etl
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Set up Google Cloud**
   - Enable BigQuery API
   - Create a dataset (e.g., `earthquake_data`)
   - Download service account key and save as:
     ```
     config/service_account.json
     ```

---

## âš™ï¸ How to Run the Pipeline

### ğŸ”„ Real-Time Mode (last 24h earthquakes)
```bash
python main.py --mode realtime
```

### ğŸ“… Historical Mode (by date range)
```bash
python main.py --mode historical --start_date 2023-07-01 --end_date 2023-07-02
```

> âš  Historical mode uses the `fdsnws` endpoint and supports querying back to early 1900s.  
> Recommended: limit to one day or paginate to avoid API limits.

---

## ğŸ“Š BigQuery Schema Example

Dataset: `earthquake_data`  
Tables:
- `realtime_events`
- `historical_events`

Essential columns:
- `id`, `place`, `mag`, `time`, `latitude`, `longitude`, `depth`
- `mag_category`, `is_significant`, `day_of_week`, `hour_of_day`

---

## ğŸ“Œ Sample Use Cases

- Build Looker Studio dashboard with trend line, heatmap, and severity maps
- Monitor significant (>5.5M) earthquakes globally
- Analyze time-based or regional patterns

---

## ğŸ” Security

Credentials are handled securely via service account stored at:
```
config/service_account.json
```
**Never commit this file to version control.**

---

## âœ… To-Do / Enhancements

- [ ] Add automatic scheduler with cron or Prefect
- [ ] Extend to support weekly/monthly historical ingestion
- [ ] Enable GCS export before BigQuery
- [ ] Alerting on significant earthquake events

---

## ğŸ“š References

- [USGS Earthquake GeoJSON Feed](https://earthquake.usgs.gov/earthquakes/feed/v1.0/geojson.php)
- [USGS Event Query API](https://earthquake.usgs.gov/fdsnws/event/1/)
- [Google BigQuery Docs](https://cloud.google.com/bigquery)
- [pandas-gbq](https://pandas-gbq.readthedocs.io/)

---

## ğŸ‘¨â€ğŸ’» Author

Developed by [Your Name] â€“ Data Engineer & Analyst  
Feel free to fork or contribute!
